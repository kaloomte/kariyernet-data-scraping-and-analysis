{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # HTML data structure\n",
    "from urllib.request import urlopen # Web client\n",
    "import re #Regular expression operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is our main page\n",
    "#in this project we web scrap job positions and more details about it\n",
    "\n",
    "my_url = 'https://www.kariyer.net/pozisyonlar' #this is our main page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opens the connection and downloads html page from url\n",
    "# parses html into a soup data structure to traverse html\n",
    "# as if it were a json data type.\n",
    "uClient = urlopen(my_url) \n",
    "page_html = uClient.read()\n",
    "uClient.close()\n",
    "soup = BeautifulSoup(page_html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first of we need more details statistics about positions\n",
    "#we need all pages for each position so we need get links from our main page\n",
    "links = []\n",
    "\n",
    "#in HTML, links are value of 'href' key\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"\")}):\n",
    "    links.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are unnecessary link\n",
    "del links[:42]\n",
    "del links[-36:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display first five links\n",
    "links[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding head of URL for searching\n",
    "add_site = 'https://www.kariyer.net'\n",
    "new_links = [add_site + j for j in links]\n",
    "new_links[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new list for searching total_participants, male and female distribution\n",
    "all_statics_add = '/nasil+olunur'\n",
    "all_statics_add_links = [n + all_statics_add for n in new_links]\n",
    "all_statics_add_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new list for searching job advertisements data\n",
    "href_text_link = []\n",
    "is_ilanlari = '/is-ilanlari'\n",
    "pozisyonlar = '/pozisyonlar'\n",
    "for k in new_links:\n",
    "    is_ilanlari_new_links = k.replace(pozisyonlar, is_ilanlari)\n",
    "    href_text_link.append(is_ilanlari_new_links)\n",
    "href_text_link[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a csv file for job advertisement number for each profession\n",
    "job_filename = 'jobs.csv'\n",
    "headers = 'job_adv.'\n",
    "f = open(job_filename, 'w')\n",
    "f.write(headers)\n",
    "#taken job advertisement number for each link via loop\n",
    "for jobadv in href_text_link:\n",
    "    uClient = urlopen(jobadv)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    #try to get number\n",
    "    try:\n",
    "        job_advertisement = page_soup.find_all('span',{'id':'totalJobCount'})[0].text\n",
    "        print('job_advertisement:' + job_advertisement)\n",
    "        f.write(job_advertisement + '\\n')\n",
    "    #there are missing number\n",
    "    except:\n",
    "        print('job_advertisement:' + '-')\n",
    "        f.write('-' + '\\n')\n",
    "    print('------------------------------------')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a csv file for job advertisement number for each profession\n",
    "filename = 'genderdistr.csv'\n",
    "headers = 'total_participants, male, female \\n' \n",
    "f = open(filename, 'w')\n",
    "f.write(headers)\n",
    "\n",
    "#taken job advertisement number for each link via loop\n",
    "for eachjob in all_statics_add_links:\n",
    "    uClient = urlopen(eachjob)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    #get datas\n",
    "    try:\n",
    "        female_distri = page_soup.findAll('div',{'class':'pg-chart-label female'})[0].text.split('\\n')[1].replace('%', '').replace(',','.')\n",
    "        male_distri = page_soup.findAll('div',{'class':'pg-chart-label male'})[0].text.split('\\n')[1].replace('%', '').replace(',', '.')\n",
    "        total_participants = page_soup.findAll('div', {'class':'pg-chart-main-text'})[0].text.split(\" \")[0]\n",
    "        print('total:' + total_participants)\n",
    "        print('female:' + female_distri)\n",
    "        print('male:' + male_distri)\n",
    "        f.write(total_participants + ', ' + male_distri + ', ' + female_distri + '\\n')\n",
    "    #for missing values\n",
    "    except:\n",
    "        f.write('-' + ', ' + '-' + ', ' + '-' + '\\n')\n",
    "        print('total:' + '-')\n",
    "        print('female:'+ '-')\n",
    "        print('male:' + '-')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_filename = 'maindata.csv'\n",
    "headers = 'job_name, mean_salary, max_salary, min_salary, first_grad, second_grad, third_grad, fourth_grad, fiveth_grad, first_department, second_department, third_department, fourth_department, fiveth_department, first_school, second_school, third_school, fourth_school, fiveth_school, first_skill, second_skill, third_skill, fourth_skill, fiveth_skill, number_of_entries \\n'\n",
    "f = open(job_filename, 'w')\n",
    "f.write(headers)\n",
    "\n",
    "for eachposition in new_links:\n",
    "    uClient = urlopen(eachposition)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "    page_soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    try:\n",
    "        job_name = page_soup.findAll('div', {'class':'pg-breadcrumb'})[0].findAll('a')[2].text\n",
    "        print('job_name:' + job_name)\n",
    "        f.write(job_name + ', ')   \n",
    "    except:\n",
    "        print('job_name:' + '-')\n",
    "        f.write('-' + ', ')\n",
    "    try:\n",
    "        mean_salary_value = page_soup.findAll('div', {'class':'pg-salary-box-value'})[0].text\n",
    "        max_salary_value = page_soup.findAll('div', {'class':'pg-salary-box-value'})[1].text\n",
    "        min_salary_value = page_soup.findAll('div', {'class':'pg-salary-box-value'})[2].text\n",
    "        print('mean_salary_value:' + mean_salary_value)\n",
    "        print('max_salary_value:' + max_salary_value)\n",
    "        print('min_salary_value:' + min_salary_value)\n",
    "        f.write(mean_salary_value + ', ' + max_salary_value + ', ' + min_salary_value + ',')\n",
    "    except:\n",
    "        print('mean_salary_value:' +'-')\n",
    "        print('max_salary_value:' + '-')\n",
    "        print('min_salary_value:' + '-')\n",
    "        f.write('-' + ', '+ '-' + ', ' + '-' + ', ')\n",
    "    try:\n",
    "        first_grad_ = page_soup.findAll('span', {'class':'pg-progress-left'})[0].text.replace(',', ' -')\n",
    "        second_grad_ = page_soup.findAll('span', {'class':'pg-progress-left'})[1].text.replace(',', ' -')\n",
    "        third_grad_ = page_soup.findAll('span', {'class':'pg-progress-left'})[2].text.replace(',', ' -')\n",
    "        fourth_grad_ = page_soup.findAll('span', {'class':'pg-progress-left'})[3].text.replace(',', ' -')\n",
    "        fiveth_grad_ = page_soup.findAll('span', {'class':'pg-progress-left'})[4].text.replace(',', ' -')\n",
    "        print('first_grad_ ' + first_grad_)\n",
    "        print('second_grad_ ' + second_grad_)\n",
    "        print('third_grad_' + third_grad_)\n",
    "        print('fourth_grad_' + fourth_grad_)\n",
    "        print('fiveth_grad_' + fiveth_grad_)\n",
    "        f.write(first_grad_ + ', ' + second_grad_ + ', ' + third_grad_ + ', ' + fourth_grad_ + ', ' + fiveth_grad_  + ', ')\n",
    "    except:\n",
    "        print('first_grad_:' + '-')\n",
    "        print('second_grad_:' + '-')\n",
    "        print('third_grad_:' + '-')\n",
    "        print('fourth_grad_:' + '-')\n",
    "        print('fiveth_grad_:' + '-')\n",
    "        f.write('-' + ', ' + '-' + ', ' + '-' + ', ' + '-' + ', ' + '-' + ', ')\n",
    "    try:\n",
    "        first_department = page_soup.findAll('span', {'class':'pg-progress-left'})[5].text\n",
    "        second_department = page_soup.findAll('span', {'class':'pg-progress-left'})[6].text\n",
    "        third_department = page_soup.findAll('span', {'class':'pg-progress-left'})[7].text\n",
    "        fourth_department = page_soup.findAll('span', {'class':'pg-progress-left'})[8].text\n",
    "        fiveth_department = page_soup.findAll('span', {'class':'pg-progress-left'})[9].text\n",
    "        print('first_departmant :'+ first_department)\n",
    "        print('second_departmant :'+ second_department)\n",
    "        print('third_departmant :' +third_department)\n",
    "        print('fourth_departmant :' +fourth_department)\n",
    "        print('fiveth_departmant :' +fiveth_department)\n",
    "        f.write(first_department + ', ' + second_department + ', ' + third_department + ', ' + fourth_department + ', ' + fiveth_department + ', ')\n",
    "    except:\n",
    "        print('first_departmant :' '-')\n",
    "        print('second_departmant :' '-')\n",
    "        print('third_departmant :' '-')\n",
    "        print('fourth_departmant :' '-')\n",
    "        print('fiveth_departmant :' '-')\n",
    "        f.write('-' + ', ' + '-' + ', ' + '-' + ', ' + '-' + ', ' + '-' + ', ')\n",
    "    try:\n",
    "        first_school = page_soup.findAll('ol', {'class':'pg-uni-list'})[0].findAll('span')[0].text\n",
    "        second_school = page_soup.findAll('ol', {'class':'pg-uni-list'})[0].findAll('span')[1].text\n",
    "        third_school = page_soup.findAll('ol', {'class':'pg-uni-list'})[0].findAll('span')[2].text\n",
    "        fourth_school = page_soup.findAll('ol', {'class':'pg-uni-list'})[0].findAll('span')[3].text\n",
    "        fiveth_school = page_soup.findAll('ol', {'class':'pg-uni-list'})[0].findAll('span')[4].text\n",
    "        print('first_school:' + first_school)\n",
    "        print('second_school:' + second_school)\n",
    "        print('third_school:' + third_school)\n",
    "        print('fourth_school:' + fourth_school)\n",
    "        print('fiveth_school:' + fiveth_school)\n",
    "        f.write(first_school + ', ' + second_school + ', ' +  third_school + ', ' + fourth_school + ', ' + fiveth_school+ ', ' )\n",
    "    except:\n",
    "        print('first_school:' + '-')\n",
    "        print('second_school:' + '-')\n",
    "        print('third_school:' + '-')\n",
    "        print('fourth_school:' + '-')\n",
    "        print('fiveth_school:' + '-')\n",
    "        f.write('-' + ', ' + '-' + ', ' + '-' + ', ' + '-' + ', ' + '-' + ', ')\n",
    "    try:\n",
    "        first_skill = page_soup.findAll('div', {'class':'pg-how-to-be-desc'})[0].findAll('span')[0].text\n",
    "        print('first_skill:' + first_skill)\n",
    "        f.write(first_skill + ',')\n",
    "    except:\n",
    "        print('first_skill:' + '-')\n",
    "        f.write('-' + ', ')\n",
    "    try:\n",
    "        second_skill = page_soup.findAll('div', {'class':'pg-how-to-be-desc'})[0].findAll('span')[1].text\n",
    "        print('second_skill :' + second_skill )\n",
    "        f.write(second_skill  + ',')\n",
    "    except:\n",
    "        print('second_skill :' + '-')\n",
    "        f.write('-' + ', ')\n",
    "    try:\n",
    "        third_skill = page_soup.findAll('div', {'class':'pg-how-to-be-desc'})[0].findAll('span')[2].text\n",
    "        print('third_skill:' + third_skill)\n",
    "        f.write(third_skill + ',')\n",
    "    except:\n",
    "        print('third_skill:' + '-')\n",
    "        f.write('-' + ', ')\n",
    "    try:\n",
    "        fourth_skill = page_soup.findAll('div', {'class':'pg-how-to-be-desc'})[0].findAll('span')[3].text\n",
    "        print('fourth_skill:' + fourth_skill)\n",
    "        f.write(fourth_skill + ',')\n",
    "    except:\n",
    "        print('fourth_skill:' + '-')\n",
    "        f.write('-' + ', ')\n",
    "    try:\n",
    "        fiveth_skill = page_soup.findAll('div', {'class':'pg-how-to-be-desc'})[0].findAll('span')[4].text\n",
    "        print('fiveth_skill:' + fiveth_skill)\n",
    "        f.write(fiveth_skill + ',')\n",
    "    except:\n",
    "        print('fiveth_skill:' + '-')\n",
    "        f.write('-' + ', ')\n",
    "    try:\n",
    "        number_of_entries = page_soup.findAll('div', {'class':'pg-salary-desc'})[0].findAll('strong')[0].text\n",
    "        print('number_of_entries:' + number_of_entries)\n",
    "        f.write(number_of_entries + ',' + '\\n')\n",
    "    except:\n",
    "        print('number_of_entries:' + '-')\n",
    "        f.write('-' + ',' + '\\n')\n",
    "    print('--------------------------------------------------------')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('maindata.txt', header=None, sep='\\n')\n",
    "df = df[0].str.split(',', expand=True)\n",
    "df.drop(df.iloc[:,-6:], axis=1, inplace=True)\n",
    "df.columns = df.iloc[0]\n",
    "df.drop(0,inplace=True)\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('jobs.txt').astype('str')\n",
    "df2['job_adv.'].replace('37.204', '0', inplace=True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read gender distribution file\n",
    "df3 = pd.read_csv('genderdistr.csv')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat all dataframe and sent into one general dataset csv file\n",
    "general_dataset = pd.concat([df, df2, df3], axis=1)\n",
    "general_dataset.to_csv('general_dataset.csv', encoding='utf-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
